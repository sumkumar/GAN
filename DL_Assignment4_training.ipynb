{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment4_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHMEDIroZSag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfFsMeEFZbbd",
        "colab_type": "code",
        "outputId": "461af473-e6f2-4b35-d1b0-c3327c27614e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LeakyReLU, BatchNormalization, ReLU\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Reshape, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers\n",
        "from keras.utils import plot_model, np_utils\n",
        "from keras import backend as K\n",
        "from SpectralNormalizationKeras import DenseSN, ConvSN2D, ConvSN2DTranspose"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6F9yZHynwBi",
        "colab_type": "code",
        "outputId": "211383ad-7366-4fe8-c593-fe6832b68899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OVOS_QdcGqu",
        "colab_type": "code",
        "outputId": "713580ce-4f19-4cf2-cf22-d1e31fb18db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq3QOmtecZbd",
        "colab_type": "code",
        "outputId": "7499b0da-a26f-4b79-a892-8b51f3497dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_classes = len(np.unique(y_train))\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 3, 32, 32)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 3, 32, 32)\n",
        "    input_shape = (3, 32, 32)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
        "    input_shape = (32, 32, 3)\n",
        "    \n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# the generator is using tanh activation, for which we need to preprocess \n",
        "# the image data into the range between -1 and 1.\n",
        "\n",
        "X_train = np.float32(X_train)\n",
        "X_train = (X_train / 255 - 0.5) * 2\n",
        "X_train = np.clip(X_train, -1, 1)\n",
        "\n",
        "X_test = np.float32(X_test)\n",
        "X_test = (X_test / 255 - 0.5) * 2\n",
        "X_test = np.clip(X_test, -1, 1)\n",
        "\n",
        "print('X_train reshape:', X_train.shape)\n",
        "print('X_test reshape:', X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train reshape: (50000, 32, 32, 3)\n",
            "X_test reshape: (50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ_yMy4ZksJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = X_train[0].shape\n",
        "init = initializers.RandomNormal(stddev=0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMayehblcc7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator(latent_dim, init):\n",
        "    model = Sequential([\n",
        "        Dense(2*2*512, input_shape=(latent_dim,), kernel_initializer=init),\n",
        "        Reshape((2, 2, 512)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh')\n",
        "    ])\n",
        "    print(\"Generator Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1T33BpLtagw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator_SN(latent_dim, init):\n",
        "    model = Sequential([\n",
        "        DenseSN(2*2*512, input_shape=(latent_dim,), kernel_initializer=init),\n",
        "        Reshape((2, 2, 512)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2DTranspose(256, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2DTranspose(64, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh')\n",
        "    ])\n",
        "    print(\"Generator Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMitagK1g7lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_8nMUu2eW0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator(img_shape, init):\n",
        "    model = Sequential([\n",
        "        Conv2D(64, kernel_size=5, strides=2, padding='same', input_shape=(img_shape), kernel_initializer=init),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2D(256, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2D(512, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Flatten(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    print(\"Discriminator Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLkF4o2Utc3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator_SN(img_shape, init):\n",
        "    model = Sequential([\n",
        "        ConvSN2D(64, kernel_size=5, strides=2, padding='same', input_shape=(img_shape), kernel_initializer=init),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2D(128, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2D(256, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2D(512, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Flatten(),\n",
        "        DenseSN(1, activation='sigmoid')\n",
        "    ])\n",
        "    print(\"Discriminator Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXhgvKcGPzKj",
        "colab_type": "code",
        "outputId": "7b4259dd-c152-4fd8-e144-c5a7d89b3079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "pip install tensorflow-gan"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 5.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 92kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 112kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 122kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 133kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 143kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 153kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 163kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 174kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 184kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 194kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 204kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 215kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 225kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 235kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 245kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 256kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 266kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 276kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 286kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 296kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 307kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 317kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 327kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 337kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 348kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 358kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan) (0.10.0rc0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub>=0.2->tensorflow-gan) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub>=0.2->tensorflow-gan) (1.18.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub>=0.2->tensorflow-gan) (3.10.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.7->tensorflow-gan) (1.3.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.7->tensorflow-gan) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.2->tensorflow-gan) (46.1.3)\n",
            "Installing collected packages: tensorflow-gan\n",
            "Successfully installed tensorflow-gan-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfS071Cbxag-",
        "colab_type": "code",
        "outputId": "73d608d3-0692-44b7-90cf-872ee3d043fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import tensorflow_gan as tfgan"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F8Corex6giX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_fid(model, images1, images2):\n",
        "    # calculate activations\n",
        "    act1 = model.predict(images1)\n",
        "    act2 = model.predict(images2)\n",
        "    fid = tfgan.eval.frechet_classifier_distance_from_activations(act1, act2)\n",
        "    return fid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CquOTQuFqSaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return K.mean(y_true*y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D7T_4DDrnlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from numpy import cov"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLAYykhwfRWY",
        "colab_type": "code",
        "outputId": "3ba08135-cc1f-4418-8183-89f9a9cb4b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "DC_generator = get_generator_SN(latent_dim, init)\n",
        "DC_discriminator = get_discriminator_SN(img_shape, init)\n",
        "DC_discriminator.compile(Adam(lr=0.0003, beta_1=0.5), loss='binary_crossentropy', metrics=['binary_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator Model Summary\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_sn_1 (DenseSN)         (None, 2048)              208896    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_transpose_1 (Conv (None, 4, 4, 256)         3277568   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_transpose_2 (Conv (None, 8, 8, 128)         819584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_transpose_3 (Conv (None, 16, 16, 64)        204992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_transpose_4 (Conv (None, 32, 32, 3)         4867      \n",
            "=================================================================\n",
            "Total params: 4,519,747\n",
            "Trainable params: 4,514,819\n",
            "Non-trainable params: 4,928\n",
            "_________________________________________________________________\n",
            "None\n",
            "Discriminator Model Summary\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_s_n2d_1 (ConvSN2D)      (None, 16, 16, 64)        4928      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_2 (ConvSN2D)      (None, 8, 8, 128)         205056    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_3 (ConvSN2D)      (None, 4, 4, 256)         819712    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv_s_n2d_4 (ConvSN2D)      (None, 2, 2, 512)         3277824   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_sn_2 (DenseSN)         (None, 1)                 2050      \n",
            "=================================================================\n",
            "Total params: 4,313,154\n",
            "Trainable params: 4,310,401\n",
            "Non-trainable params: 2,753\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O4p1PDUgO3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_model(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    z = Input(shape=(latent_dim,))\n",
        "    img = generator(z)\n",
        "    decision = discriminator(img)\n",
        "    model = Model(inputs=z, outputs=decision)\n",
        "    model.compile(Adam(lr=0.0004, beta_1=0.5), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "    print(\"GAN Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgUroT5Si8bv",
        "colab_type": "code",
        "outputId": "2fd017d5-e322-4cb8-8aa7-13f70889a11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "dcgan_model = get_gan_model(DC_generator, DC_discriminator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GAN Model Summary\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 32, 32, 3)         4519747   \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 1)                 4313154   \n",
            "=================================================================\n",
            "Total params: 8,832,901\n",
            "Trainable params: 4,514,819\n",
            "Non-trainable params: 4,318,082\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKkbJZhIhxvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_train(gan_model, generator, discriminator, epochs, batch_size, smooth, show_train):\n",
        "    real = np.ones(shape=(batch_size, 1))\n",
        "    fake = np.zeros(shape=(batch_size, 1))\n",
        "\n",
        "    d_loss = []\n",
        "    g_loss = []\n",
        "    for e in range(epochs + 1):\n",
        "        for i in range(len(X_train) // batch_size):\n",
        "            \n",
        "            # Train Discriminator weights\n",
        "            discriminator.trainable = True\n",
        "            \n",
        "            # Real samples\n",
        "            X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
        "            d_loss_real = discriminator.train_on_batch(x=X_batch, y=real * (1 - smooth))\n",
        "            \n",
        "            # Fake Samples\n",
        "            z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
        "            X_fake = generator.predict_on_batch(z)\n",
        "            d_loss_fake = discriminator.train_on_batch(x=X_fake, y=fake)\n",
        "            \n",
        "            # Wasserstein loss\n",
        "            d_loss_batch = tf.reduce_mean(d_loss_fake) - tf.reduce_mean(d_loss_real)\n",
        "            \n",
        "            # Train Generator weights\n",
        "            discriminator.trainable = False\n",
        "            g_loss_batch = gan_model.train_on_batch(x=z, y=real)\n",
        "\n",
        "            print(\n",
        "                'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, g_loss_batch[0]),\n",
        "                100*' ',\n",
        "                end='\\r'\n",
        "            )\n",
        "        \n",
        "        d_loss.append(d_loss_batch)\n",
        "        g_loss.append(g_loss_batch[0])\n",
        "        print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], g_loss[-1]), 100*' ')\n",
        "        if show_train == True:\n",
        "            #if e % 10 == 0:\n",
        "            samples = 10\n",
        "            x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, latent_dim)))\n",
        "\n",
        "            for k in range(samples):\n",
        "                plt.subplot(2, 5, k + 1, xticks=[], yticks=[])\n",
        "                plt.imshow(((x_fake[k] + 1)* 127).astype(np.uint8))\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    return d_loss, g_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z7zqc6QOWE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def self_attention( x, r = 8, reuse = False ) :\n",
        "    [ _, h, w, c ] = x.get_shape().as_list()\n",
        "    n = h * w\n",
        "    cr = c // r\n",
        "    pf = ConvSN2D( x, cr, 1, 1)\n",
        "    pg = ConvSN2D( x, cr, 1, 1)\n",
        "    ph = ConvSN2D( x, c, 1, 1)\n",
        "    f = tf.reshape( pf, [ -1, n, cr ] ) \n",
        "    g = tf.reshape( tf.transpose( pg, [ 0, 3, 1, 2 ] ), [ -1, cr, n ] ) \n",
        "    h_ = tf.reshape( ph, [ -1, n, c ] ) \n",
        "    fg = tf.matmul( f, g ) # n by n matrix\n",
        "    fg = tf.nn.softmax( fg, axis = 1 ) \n",
        "    att = tf.matmul( fg, h_ ) # n by c matrix\n",
        "    att_fm = tf.reshape( att, [ -1, h, w, c ] ) \n",
        "    y = tf.get_variable( 'y', shape = [], dtype = tf.float32, initializer=tf.constant_initializer( 0 ) ) \n",
        "    return x + y * att_fm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBo19nUykC6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters\n",
        "epochs = 1000\n",
        "batch_size = 4096\n",
        "smooth = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRslDAk1jz3H",
        "colab_type": "code",
        "outputId": "ce0099f2-8b4e-4212-c723-df048da472e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "d_loss, g_loss = gan_train(dcgan_model, DC_generator, DC_discriminator, epochs, batch_size, smooth, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 1/1000, d_loss=0.311, g_loss=6.933                                                                                                     \n",
            "epoch = 2/1000, d_loss=0.331, g_loss=0.004                                                                                                     \n",
            "epoch = 3/1000, d_loss=0.329, g_loss=0.002                                                                                                     \n",
            "epoch = 4/1000, d_loss=0.329, g_loss=0.001                                                                                                     \n",
            "epoch = 5/1000, d_loss=0.332, g_loss=0.001                                                                                                     \n",
            "epoch = 6/1000, d_loss=0.332, g_loss=0.001                                                                                                     \n",
            "epoch = 7/1000, d_loss=0.334, g_loss=0.001                                                                                                     \n",
            "epoch = 8/1000, d_loss=0.334, g_loss=0.001                                                                                                     \n",
            "epoch = 9/1000, d_loss=0.335, g_loss=0.001                                                                                                     \n",
            "epoch = 10/1000, d_loss=0.336, g_loss=0.001                                                                                                     \n",
            "epoch = 11/1000, d_loss=0.336, g_loss=0.001                                                                                                     \n",
            "epoch = 12/1000, d_loss=0.336, g_loss=0.001                                                                                                     \n",
            "epoch = 13/1000, d_loss=0.336, g_loss=0.001                                                                                                     \n",
            "epoch = 14/1000, d_loss=0.336, g_loss=0.001                                                                                                     \n",
            "epoch = 15/1000, d_loss=0.336, g_loss=0.001                                                                                                     \n",
            "epoch = 16/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 17/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 18/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 19/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 20/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 21/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 22/1000, d_loss=0.337, g_loss=0.001                                                                                                     \n",
            "epoch = 23/1000, d_loss=0.337, g_loss=0.002                                                                                                     \n",
            "epoch = 24/1000, d_loss=0.337, g_loss=0.002                                                                                                     \n",
            "epoch = 25/1000, d_loss=0.337, g_loss=0.003                                                                                                     \n",
            "epoch = 26/1000, d_loss=-2.029, g_loss=3.125                                                                                                     \n",
            "epoch = 27/1000, d_loss=0.351, g_loss=4.410                                                                                                     \n",
            "epoch = 28/1000, d_loss=0.335, g_loss=0.502                                                                                                     \n",
            "epoch = 29/1000, d_loss=0.296, g_loss=2.438                                                                                                     \n",
            "epoch = 30/1000, d_loss=0.363, g_loss=2.132                                                                                                     \n",
            "epoch = 31/1000, d_loss=0.354, g_loss=1.537                                                                                                     \n",
            "epoch = 32/1000, d_loss=0.411, g_loss=2.501                                                                                                     \n",
            "epoch = 33/1000, d_loss=0.381, g_loss=1.745                                                                                                     \n",
            "epoch = 34/1000, d_loss=0.366, g_loss=1.335                                                                                                     \n",
            "epoch = 35/1000, d_loss=0.368, g_loss=1.399                                                                                                     \n",
            "epoch = 36/1000, d_loss=0.599, g_loss=5.286                                                                                                     \n",
            "epoch = 37/1000, d_loss=0.321, g_loss=0.676                                                                                                     \n",
            "epoch = 38/1000, d_loss=0.129, g_loss=7.171                                                                                                     \n",
            "epoch = 39/1000, d_loss=0.298, g_loss=3.908                                                                                                     \n",
            "epoch = 40/1000, d_loss=0.351, g_loss=2.551                                                                                                     \n",
            "epoch = 41/1000, d_loss=0.335, g_loss=1.338                                                                                                     \n",
            "epoch = 42/1000, d_loss=0.369, g_loss=1.994                                                                                                     \n",
            "epoch = 43/1000, d_loss=0.457, g_loss=6.881                                                                                                     \n",
            "epoch = 44/1000, d_loss=0.334, g_loss=0.068                                                                                                     \n",
            "epoch = 45/1000, d_loss=0.335, g_loss=0.033                                                                                                     \n",
            "epoch = 46/1000, d_loss=0.336, g_loss=0.028                                                                                                     \n",
            "epoch = 47/1000, d_loss=0.336, g_loss=0.029                                                                                                     \n",
            "epoch = 48/1000, d_loss=0.337, g_loss=0.039                                                                                                     \n",
            "epoch = 49/1000, d_loss=0.337, g_loss=0.046                                                                                                     \n",
            "epoch = 50/1000, d_loss=0.337, g_loss=0.045                                                                                                     \n",
            "epoch = 51/1000, d_loss=0.336, g_loss=0.036                                                                                                     \n",
            "epoch = 52/1000, d_loss=0.336, g_loss=0.044                                                                                                     \n",
            "epoch = 53/1000, d_loss=0.327, g_loss=0.047                                                                                                     \n",
            "epoch = 54/1000, d_loss=0.329, g_loss=0.046                                                                                                     \n",
            "epoch = 55/1000, d_loss=0.328, g_loss=0.109                                                                                                     \n",
            "epoch = 56/1000, d_loss=0.354, g_loss=2.750                                                                                                     \n",
            "epoch = 57/1000, d_loss=0.348, g_loss=2.936                                                                                                     \n",
            "epoch = 58/1000, d_loss=0.349, g_loss=2.772                                                                                                     \n",
            "epoch = 59/1000, d_loss=0.334, g_loss=0.090                                                                                                     \n",
            "epoch = 60/1000, d_loss=0.336, g_loss=0.057                                                                                                     \n",
            "epoch = 61/1000, d_loss=0.336, g_loss=0.054                                                                                                     \n",
            "epoch = 62/1000, d_loss=0.336, g_loss=0.053                                                                                                     \n",
            "epoch = 63/1000, d_loss=0.336, g_loss=0.038                                                                                                     \n",
            "epoch = 64/1000, d_loss=0.335, g_loss=0.032                                                                                                     \n",
            "epoch = 65/1000, d_loss=0.335, g_loss=0.021                                                                                                     \n",
            "epoch = 66/1000, d_loss=0.335, g_loss=0.016                                                                                                     \n",
            "epoch = 67/1000, d_loss=0.335, g_loss=0.017                                                                                                     \n",
            "epoch = 68/1000, d_loss=0.336, g_loss=0.042                                                                                                     \n",
            "epoch = 69/1000, d_loss=0.166, g_loss=2.958                                                                                                     \n",
            "epoch = 70/1000, d_loss=0.401, g_loss=3.641                                                                                                     \n",
            "epoch = 71/1000, d_loss=0.344, g_loss=3.690                                                                                                     \n",
            "epoch = 72/1000, d_loss=0.335, g_loss=0.030                                                                                                     \n",
            "epoch = 73/1000, d_loss=0.336, g_loss=0.021                                                                                                     \n",
            "epoch = 74/1000, d_loss=0.336, g_loss=0.017                                                                                                     \n",
            "epoch = 75/1000, d_loss=0.337, g_loss=0.017                                                                                                     \n",
            "epoch = 76/1000, d_loss=0.337, g_loss=0.019                                                                                                     \n",
            "epoch = 77/1000, d_loss=0.338, g_loss=0.029                                                                                                     \n",
            "epoch = 78/1000, d_loss=-0.801, g_loss=7.446                                                                                                     \n",
            "epoch = 79/1000, d_loss=0.355, g_loss=1.078                                                                                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGp-Lrw0Yhr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DC_generator.save('./DC_generator_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFo3Qi7uZI6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DC_discriminator.save('./DC_discriminator_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISbsWBFBZTPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator.predict(None, X_test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Py-62gm2mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh_hdZDFoZTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_fake = DC_generator.predict(np.random.normal(loc=0, scale=1, size=(1, latent_dim)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7fCRo28Yab6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in range(1):\n",
        "    plt.subplot(2, 5, k + 1, xticks=[], yticks=[])\n",
        "    plt.imshow(((x_fake[k] + 1)* 127).astype(np.uint8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQeL5GNcrA0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1 = []\n",
        "x1.append(X_test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvFuM545LP1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fid1 = calculate_fid(DC_discriminator, x_fake, x1)\n",
        "print(fid1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNF9xN-iizOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_fake = DC_generator.predict(np.random.normal(loc=0, scale=1, size=(1, latent_dim)))\n",
        "#x_fake = np.asarray(x_fake)\n",
        "#x1 = np.asarray(x1)\n",
        "fid1 = calculate_fid(DC_discriminator, x_fake, x1)\n",
        "print(fid1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK4G7x--ZM3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calculate_fid(dcgan_model, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCMgg61xTsDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-self-attention\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTq72Di8ROZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator_SN_SA(latent_dim, init):\n",
        "    model = Sequential([\n",
        "        DenseSN(2*2*512, input_shape=(latent_dim,), kernel_initializer=init),\n",
        "        Reshape((2, 2, 512)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2DTranspose(256, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        AttentionLayer((None, 8, 8, 128)),\n",
        "        ConvSN2DTranspose(64, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        AttentionLayer((None, 32, 32, 3)),\n",
        "        ConvSN2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh')\n",
        "    ])\n",
        "    print(\"Generator Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VYyiEWtRrt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator_SN_SA(img_shape, init):\n",
        "    model = Sequential([\n",
        "        ConvSN2D(64, kernel_size=5, strides=2, padding='same', input_shape=(img_shape), kernel_initializer=init),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2D(128, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        ConvSN2D(256, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        SeqSelfAttention(attention_activation='sigmoid'),\n",
        "        ConvSN2D(512, kernel_size=5, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        SeqSelfAttention(attention_activation='sigmoid'),\n",
        "        Flatten(),\n",
        "        DenseSN(1, activation='sigmoid')\n",
        "    ])\n",
        "    print(\"Discriminator Model Summary\")\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHMxHezSItd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SA_generator = get_generator_SN_SA(latent_dim, init)\n",
        "SA_discriminator = get_discriminator_SN_SA(img_shape, init)\n",
        "SA_discriminator.compile(Adam(lr=0.0003, beta_1=0.5), loss='binary_crossentropy', metrics=['binary_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzZ-RPzNSVQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sagan_model = get_gan_model(SA_generator, SA_discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9eUwrVEShrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_train(sagan_model, SA_generator, SA_discriminator, epochs, batch_size, smooth, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ7pzcVGYq_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(d_loss)\n",
        "plt.plot(g_loss)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Discriminator', 'Adversarial'], loc='center right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}